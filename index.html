<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="description" content="Bimanual Imitation">
  <meta name="keywords" content="Imitation Learning, Bimanual Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Bimanual Imitation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->

  <!-- (Remove if cloning) !! Tag for bimanual imitation !! -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VE0696DKLE"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-VE0696DKLE');
  </script>
  <!-- (Remove if cloning) !! Tag for bimanual imitation !! -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/irl_lab.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <style>
    body,
    html {
      margin: 0;
      padding: 0;
      height: 100%;
      overflow: auto;
    }
  </style>

  <link rel="stylesheet" href="styles.css">

</head>

<body>
  <header class="banner">
    <div class="banner">
      <video autoplay loop muted playsinline oncontextmenu="return false;" preload="auto" id="bannerVideo">
        <source src="static/videos/banner.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>

      <div class="overlay">
        <h1 class="overlay-h1">A Comparison of Imitation Learning Algorithms for Bimanual Manipulation</h1>
      </div>
    </div>
    <div class="down-arrow" onclick="scrollToNextSection()">
      <div class="v-shape-left"></div>
      <div class="v-shape-right"></div>
    </div>

  </header>

  <section class="hero" id="main-section">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">A Comparison of Imitation Learning Algorithms for Bimanual
              Manipulation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://drolet.io/">Michael Drolet</a><sup>1,4</sup>,</span>
              <span class="author-block">
                <a href="https://simonstepputtis.com/">Simon Stepputtis</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://skailas.github.io/">Siva Kailas </a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://jainajinkya.github.io/">Ajinkya Jain</a> <sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.ias.informatik.tu-darmstadt.de/Member/JanPeters">Jan Peters</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://stefan-schaal.net/">Stefan Schaal</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="http://henibenamor.weebly.com/">Heni Ben Amor</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-unis">
              <span class="author-block"><sup>1</sup><a
                  href="https://interactive-robotics.engineering.asu.edu/">Interactive Robotics Lab, Arizona State
                  University</span></a><br>
              <span class="author-block"><sup>2</sup><a href="https://www.ri.cmu.edu/"> The Robotics Institute, Carnegie Mellon
                  University</span></a><br>
              <span class="author-block"><sup>3</sup><a href="https://www.intrinsic.ai/"> Intrinsic AI (An Alphabet Company)</span></a><br>
              <span class="author-block"><sup>4</sup><a href="https://www.ias.informatik.tu-darmstadt.de/"> Intelligent Autonomous Systems Lab, TU
                  Darmstadt</span></a>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="./static/videos/task_sequence.mp4" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/ir-lab/bimanual-imitation" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <!-- <span class="link-block">
                  <a href="https://www.dropbox.com/scl/fo/2onj92s7gewettu1rjg3d/h?rlkey=1d4dnhwf3z6s4a51lopgocby5&dl=0"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span> -->
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Amidst the wide popularity of imitation learning algorithms in robotics, their properties regarding
              hyperparameter sensitivity, ease of training, data efficiency, and performance have not been well-studied
              in high-precision industry-inspired environments. In this work, we demonstrate the limitations and
              benefits of prominent imitation learning approaches and analyze their capabilities regarding these
              properties. We evaluate each algorithm on a complex bimanual manipulation task involving an
              over-constrained dynamics system in a setting involving multiple contacts between the manipulated object
              and the environment. While we find that imitation learning is well suited to solve such complex tasks, not
              all algorithms are equal in terms of handling environmental and hyperparameter perturbations, training
              requirements, performance, and ease of use. We investigate the influence of these key characteristics by
              employing a carefully designed experimental procedure and learning environment.
          </div>
        </div>
      </div>

      <!-- Abstract. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
              Our experiments indicate that recent Imitation Learning methods, such as Diffusion and ACT, outperform
              traditional techniques (with respect to various metrics) on our high-precision adapter insertion task
              without the need for extensive hyperparameter tuning. The policy architectures proposed by ACT and
              Diffusion are effective when combined with action and observation chunking, helping to cope with the noisy
              (potentially non-Markovian) environment scenario. Interestingly, the online methods- DAgger and GAIL-
              exhibit high performance under noise perturbations without performing action and observation chunking.
              Such a result points to the important role these methods still play and demonstrates their ability to
              recover from unseen states via exploring during training. However, the need for extensive environment
              interaction (GAIL) and a pre-defined oracle (DAgger) impose real-world challenges that future works must
              address to make them more practical. Finally, we present a carefully designed environment that can serve
              as (i) a future benchmark for robot learning methods and (ii) a reference for practitioners in the
              industry.
          </div>
        </div>
      </div>

      <div class="container is-max-desktop">
        <div class="hero-body">

          <center>
            <img src="static/images/spider.png" width="80%" style="border: 5px solid rgb(245, 245, 245);" />
          </center>

          <h4 class="subtitle has-text-centered" style="font-size: 1em;">
            <br>
            A high-level overview of the key metrics and results from our study.
          </h4>
        </div>
      </div>

      <h2 class="title is-3">Learned Policies</h2>
      <div class="content has-text-justified" style="margin-bottom: 0px;">
        <p>
          Every method is trained using a subset of expert demonstrations (either 50, 100, or 200), 10 random seeds, and
          in three different environments (Zero Noise, Low Noise, or High Noise). The policies during training are
          presented below. Row 1 corresponds to 10% of training completion; row 2 corresponds to 50% of completion;
          and row 3 corresponds to 100% completion. Column 1 corresponds to the Zero Noise environment; column 2
          corresponds to the Low Noise environment; and column 3 corresponds to the High Noise environment, where noise is applied to both the observations and actions.
        </p>
      </div>
      <center>
        <div class="button-container top" data-index="0" data-variable="currentAlgorithmIndex">
          <span class="button-text">
            <pre class="cbutton"></pre>
          </span>
          <div class="button-overlay left" onclick="updateGlobalIndex(this)"></div>
          <div class="button-overlay right" onclick="updateGlobalIndex(this)"></div>
        </div>

        <div class="button-container middle" data-index="0" data-variable="currentTauIndex">
          <span class="button-text">
            <pre class="cbutton"></pre>
          </span>
          <div class="button-overlay left" onclick="updateGlobalIndex(this)"></div>
          <div class="button-overlay right" onclick="updateGlobalIndex(this)"></div>
        </div>

        <div class="button-container bottom" data-index="0" data-variable="currentSeedIndex">
          <span class="button-text">
            <pre class="cbutton"></pre>
          </span>
          <div class="button-overlay left" onclick="updateGlobalIndex(this)"></div>
          <div class="button-overlay right" onclick="updateGlobalIndex(this)"></div>
        </div>
        <br>
        <div class="container3">
          <div class="grid">
            <!-- The grid items will be populated dynamically -->
          </div>
        </div>
        <!-- <pre id="display-info" class="display-info" style="background-color: inherit; font-size: 1em; padding: 10px; margin: 10px;"></pre> -->
      </center>
      <br>
      
      <div class="content has-text-justified" style="margin-bottom: 0px;">
        <p>
          It can be seen that vanilla Behavioral Cloning (BC), Implicit Behavioral Cloning (IBC), and DAgger struggle in terms of training stability, having relatively inconsistent performance over time. However, in the default environment (Zero Noise with 200 expert deomonstrations) the performance is often satisfactory when choosing the best policy out of 10 evenly-spaced checkpoints. On the other hand, ACT and Diffusion exhibit stable behavior throughout the training process and, comparatively, quickly obtain good policy parameters. Notably, the GAIL policies exhibit strong performance toward the end of training- in all environments, once the discriminator and generator/policy reach a stable state. However, the amount of time required for this to happen is significantly higher than the other methods.
        </p>
      </div>
      <br>

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Material</h2>
          <div class="content has-text-justified" style="margin: 0px; padding-bottom: 0px;">
            <p>
              <a href="https://github.com/ir-lab/bimanual-imitation">Our codebase</a> inherits features from <a
                href="https://github.com/ir-lab/irl_control">IRL Control</a>, which supports:
            <ul>
              <li><strong>Low-level Controllers</strong> including Operational Space Control and Admittance Control.
              <li><strong>Demo Collection</strong> with PS Move controllers and 3D Connexion Space Mouse for
                teleoperation.
              </li>
              <li><strong>Configuration Files</strong> for tuning PID Gains, min/max velocities, and adding kinematic
                descriptions of the robot devices.
              </li>
            </ul>
            As our work focuses on learning in the operational/task space, we extend IRL Control to implicitly control the robot's torso through the nullspace. Thus, the action space can be characterized by using only the change in position/rotation of two end effectors. Some of the key features available are described below.
            </p>
          </div>
      <!--/ method. -->
      <div class="columns is-centered has-text-centered">
        <!-- Regid Body motion -->
        <div class="column">
          <div class="content">
            <h2 class="title is-5" style="padding: 0px; margin: 0.5em;">Data Collection and Teleoperation</h2>
            <img src="static/gifs/ps_move_demo.gif" type="video/mp4">
            <p style="text-align: justify;">
              The user teleoperates the robot for picking up objects in the scene. PS Move controllers allow for opening/closing the gripper and selecting a target which the robot moves to when the trigger is pressed. This setup can be used for collecting expert demonstrations, especially if the task is not sufficiently difficult.
            </p>
          </div>
        </div>
        <!--/ Regid Body motion -->

        <!-- Soft robot motion -->
        <div class="column">
          <div class="content">
            <h2 class="title is-5" style="padding: 0px; margin: 0.5em;">Force/Torque Sensing</h2>
            <img src="static/gifs/force_test.gif" type="video/mp4">
            <p style="text-align: justify;">
              This example makes use of the robot's force/torque to demonstrate how the admittance controller reacts to the external environment. 
              The target position of the left arm is given, such that, upon hitting the wall, the arm must gently bounce backward insteading of pushing against the wall with a larger force to reach the desired position.
            </p>
          </div>
        </div>
        <!--/ soft robot motion -->
      </div>
      <!--/ Matting. -->


      <!--/ method. -->
      <div class="columns is-centered has-text-centered">
        <!-- Regid Body motion -->
        <div class="column">
          <div class="content" style="padding-top: 0px;">
            <h2 class="title is-5" style="padding: 0px; margin: 0.5em;">Single Arm Insertion Tasks</h2>
            <img src="static/gifs/insertion_task.gif" type="video/mp4">
            <p style="text-align: justify;">
              The robot performs adapter insertion, with each component spawned at a randomly generated angle and position. The robot must precisely align and insert the components under a small tolerance and execute a sequence of actions defined by a YAML file. This file defines a generic algorithm that can be executed on either arm, as well as: how much force the gripper should apply at each stage of the task, the velocity constraints, and the objects involved in the action (containing information on the proper location and orientation for picking/inserting).              
            </p>
          </div>
        </div>
        <!--/ Regid Body motion -->

        <!-- Soft robot motion -->
        <div class="column">
          <div class="content" style="padding-top: 0px;">
            <h2 class="title is-5" style="padding: 0px; margin: 0.5em;">PID Gain Tuning</h2>
            <img src="static/gifs/gain_test.gif" type="video/mp4">
            <p style="text-align: justify;">
              This example evaluates the PID gains to ensure that the torso (which is explicitly controlled in this scenario) and passive arm are stable under high torques from the moving arm. It can be seen that the right arm is vigorously moving back-and-forth in order to apply a high torque to the rest of the robot's body. The torso and left arm are stable under these forces, suggesting that the chosen gains are adequate for tasks requiring a high-speed arm movement.           
            </p>
          </div>
        </div>
        <!--/ soft robot motion -->
      </div>
      <!--/ Matting. -->


        </div>
    </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{drolet2024,
        title={A Comparison of Imitation Learning Algorithms for Bimanual Manipulation},
        author={TODO}
      }</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <!-- <a class="icon-link" href="">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a class="icon-link" href="https://github.com/mdrolet01" class="external-link" disabled>
            <i class="fab fa-github"></i>
          </a> -->
          <div class="content">
            <p>
              This website uses the template from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <script>
    document.addEventListener('DOMContentLoaded', function () {
      function scrollToNextSection() {
        const nextSection = document.getElementById('main-section');
        if (nextSection) {
          nextSection.scrollIntoView({ behavior: 'smooth' });
        }
      }

      // Attach the function to the window object so it can be called from the onclick attribute
      window.scrollToNextSection = scrollToNextSection;
    });
  </script>

  <script>
    const video = document.getElementById('bannerVideo');
    const bannerText = document.querySelector('.overlay h1');

    video.addEventListener('timeupdate', () => {
      const currentTime = video.currentTime;
      const duration = video.duration;
      const percentage = (currentTime / duration) * 100;

      if (percentage >= 75) {
        // Normalize the percentage to a value between 0 and 1
        const normalizedOpacity = (percentage - 75) / 25; // Maps 80-100% to 0-1
        bannerText.style.opacity = normalizedOpacity.toString();
      } else {
        bannerText.style.opacity = '0'; // Ensure invisible before 80%
      }
    });
  </script>

  <script src="script.js"></script>


</body>

</html>